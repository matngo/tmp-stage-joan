{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest Path between two entries of Wikipedia's Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # That's to get the links from a wikipedia page\n",
    "import os, time\n",
    "import numpy as np\n",
    "import networkx as nx # Networkx is more convenient to analyze graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get all child links from a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_links(title, language, plcontinue=None):\n",
    "    url = \"https://{lang}.wikipedia.org/w/api.php\".format(lang=language)\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"links\",\n",
    "        \"pllimit\": \"max\",\n",
    "        \"plnamespace\": 0,\n",
    "    }\n",
    "    if plcontinue:\n",
    "        params[\"plcontinue\"] = plcontinue\n",
    "\n",
    "    data = requests.get(url, params=params)\n",
    "    data = data.json()\n",
    "    plcontinue = data.get(\"continue\", {}).get(\"plcontinue\", None)\n",
    "    for items in data[\"query\"][\"pages\"].values():\n",
    "        for link in items.get(\"links\", []):\n",
    "            yield link[\"title\"]\n",
    "\n",
    "    if plcontinue is not None:\n",
    "        yield from all_links(title, language, plcontinue=plcontinue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to clean the children links from a page from \"rubbish\", but this isn't used because of the\n",
    "difficulty of defining \"rubbish\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entries(rawlist):\n",
    "    ''' This function takes a generator as input ! \n",
    "        Clears the entries starting with a '.', as these are language links.\n",
    "    '''\n",
    "    rawlist = np.array(list(rawlist))\n",
    "    L = rawlist.size\n",
    "    \n",
    "    if L == 0:\n",
    "        return rawlist\n",
    "    else:\n",
    "        lowcut = 0\n",
    "        while (rawlist[lowcut][0] == \".\"):\n",
    "            lowcut += 1\n",
    "            if lowcut == L:\n",
    "                return np.array([])\n",
    "                \n",
    "        return rawlist[lowcut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the network(s) by doing a BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_network(language, initial_entry, target_entry, restrictions_dic):\n",
    "    ''' Inputs :\n",
    "            - language -> [string]; either \"en\", \"fr\", or \"de\", language we browse wikipedia in,\n",
    "            - initial_entry -> [string]; Wikipedia page we start the exploration from,\n",
    "            - target_entry -> [string]; Wikipedia page we want to reach,\n",
    "            - restrictions -> [dictionary]; restrictions to avoid browsing the whole network\n",
    "        Outputs :\n",
    "            - A networkx newtork object where the nodes are Wikipedia pages and the edges the links between them.\n",
    "        '''\n",
    "    \n",
    "    # Strings used for interactive output\n",
    "    outstr1 = r\"*–> Took {} (total {}) seconds to process 500 more nodes (total {}) !\"\n",
    "    outstr2 = r\"*–> Currently at node {}, layer {}, still {} to go !\"\n",
    "    \n",
    "    # Recover the restrictions\n",
    "    maxdepth = restrictions_dic[\"max_depth\"]\n",
    "    maxnodes = restrictions_dic[\"max_nodes\"]\n",
    "    \n",
    "    # Initialize the BFS\n",
    "    u = initial_entry # node we start the exploration from\n",
    "    queue = [u] # the queue of nodes to be processed\n",
    "    n = 0 # order in which the nodes are added to the queue and thus the graph\n",
    "    nprocessed = 0 # nb of processed nodes\n",
    "    \n",
    "    # Initialize the networkx network\n",
    "    G = nx.DiGraph()\n",
    "    G.add_node(u)\n",
    "    G.nodes[u][\"order\"] = n\n",
    "    G.nodes[u][\"layer\"] = 0\n",
    "    G.nodes[u][\"parent\"] = u\n",
    "    \n",
    "    # Run a BFS for as many layers as needed\n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "    while (queue != []):\n",
    "        # Select a new source node\n",
    "        u = queue[0]\n",
    "        u_neighbors = all_links(u, language) # note that this is a generator !\n",
    "        \n",
    "        # A little message to wait before the graph finishes completing once we've seen enough nodes\n",
    "        if (nprocessed == maxnodes) and (G.nodes[u][\"order\"] % 100 == 0):           \n",
    "                        print(r\"|\")\n",
    "                        print(outstr2.format(G.nodes[u][\"order\"],  G.nodes[u][\"layer\"], len(queue)))  \n",
    "            \n",
    "        # Explore the neighbors of the new source.\n",
    "        # Note that if the generator is empty then nothing happens and \"queue\" is poped -> good !\n",
    "        for v in u_neighbors:\n",
    "            # Check if the node has already been seen (and if the node is not a language link)\n",
    "            if (v not in G.nodes()) and (v[0] != \".\"):\n",
    "                # End the process if we reached the desired node\n",
    "                if v == target_entry:\n",
    "                    print(\"Finished !\")\n",
    "                    return G\n",
    "                # Check if we've attained the maximum amount of explorable nodes or the maximum allowed depth\n",
    "                if (nprocessed < maxnodes) and (G.nodes[u][\"layer\"] < maxdepth):\n",
    "                    G.add_edge(u, v)\n",
    "                    G.nodes[v][\"order\"] = n + 1\n",
    "                    G.nodes[v][\"layer\"] = G.nodes[u][\"layer\"] + 1\n",
    "                    G.nodes[v][\"parent\"] = u\n",
    "                    queue.append(v)\n",
    "                    n += 1\n",
    "                    nprocessed += 1\n",
    "\n",
    "                    # A little message every time we process more than 500 nodes\n",
    "                    if (nprocessed % 10000 == 0) and (nprocessed > 0):           \n",
    "                        t2 = time.time()\n",
    "                        deltat = t2 - t1\n",
    "                        print(r\"|\")\n",
    "                        print(outstr1.format(deltat, (t2 - t0), nprocessed))\n",
    "                        print(outstr2.format(G.nodes[u][\"order\"],  G.nodes[u][\"layer\"], len(queue)))\n",
    "                        t1 = time.time()\n",
    "                              \n",
    "        # Dequeue the node once all its neighbors have been seen\n",
    "        queue.pop(0)\n",
    "    \n",
    "    t3 = time.time()\n",
    "    deltat = t3 - t0\n",
    "    print(\"Took {} seconds to build the whole network !\".format(deltat))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictions = {\"max_depth\" : 10, # How deep in the \"BFS\" we are allowed to go\n",
    "                \"max_nodes\" : int(2e6), # Maximum of nodes EXPLORED (and not necessarily part of the network)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding shortest path from base entry \"France\" to target entry \"Germany\"\n",
      "Finished !\n",
      "Saving network ...\n"
     ]
    }
   ],
   "source": [
    "# Define the two nodes whose distance we want to find\n",
    "base_entry = \"France\"\n",
    "target_entry = \"Germany\"\n",
    "#TODO -> It might be useful to define a function that checks if those entries are true Wikipedia pages \n",
    "#    (in particular to avoid spelling mistakes making a page impossible to reach.)\n",
    "\n",
    "# Define a path to which the graph will be written\n",
    "cwd = os.getcwd()\n",
    "basepath = \"{cwd}/network-{baseentry}-{targetentry}_{maxnodes}-{maxlayer}.{ext}\"\n",
    "\n",
    "# Explore the Network\n",
    "language = \"en\"\n",
    "print()\n",
    "print(\"Finding shortest path from base entry \\\"{}\\\" to target entry \\\"{}\\\"\".format(base_entry, target_entry))\n",
    "N = explore_network(language, base_entry, target_entry, restrictions)\n",
    "\n",
    "# Save the network to different file formats\n",
    "print(\"Saving network ...\")\n",
    "filename = basepath.format(cwd=cwd, baseentry=base_entry, targetentry=target_entry,\n",
    "                           maxnodes=restrictions[\"max_nodes\"],\n",
    "                           maxlayer=restrictions[\"max_depth\"], \n",
    "                           ext=\"gml\")\n",
    "nx.write_gml(N, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
